{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    ")\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    # add_messages is a reducer\n",
    "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \"\"\"Call to get the weather from a specific location.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    # Don't let the LLM know this though ðŸ˜Š\n",
    "    if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n",
    "        return \"It's sunny in San Francisco, but you better look out if you're a Gemini ðŸ˜ˆ.\"\n",
    "    else:\n",
    "        return f\"I am not sure what the weather is in {location}\"\n",
    "\n",
    "\n",
    "tools = [get_weather]\n",
    "\n",
    "model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "# Define our tool node\n",
    "def tool_node(state: AgentState):\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "# Define the node that calls the model\n",
    "def call_model(\n",
    "    state: AgentState,\n",
    "    config: RunnableConfig,\n",
    "):\n",
    "    # this is similar to customizing the create_react_agent with 'prompt' parameter, but is more flexible\n",
    "    system_prompt = SystemMessage(\n",
    "        \"You are a helpful AI assistant, please respond to the users query to the best of your ability!\"\n",
    "    )\n",
    "    response = model.invoke([system_prompt] + state[\"messages\"], config)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the conditional edge that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Type,\n",
    "    TypeVar,\n",
    "    Union,\n",
    "    cast,\n",
    ")\n",
    "\n",
    "from langchain_core.language_models import BaseChatModel, LanguageModelLike\n",
    "from langchain_core.messages import AIMessage, BaseMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableBinding,\n",
    "    RunnableConfig,\n",
    ")\n",
    "from langchain_core.tools import BaseTool\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "from langgraph.errors import ErrorCode, create_error_message\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.managed import IsLastStep, RemainingSteps\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.types import Checkpointer, Send\n",
    "from langgraph.utils.runnable import RunnableCallable\n",
    "\n",
    "StructuredResponse = Union[dict, BaseModel]\n",
    "StructuredResponseSchema = Union[dict, type[BaseModel]]\n",
    "F = TypeVar(\"F\", bound=Callable[..., Any])\n",
    "\n",
    "\n",
    "# We create the AgentState that we will pass around\n",
    "# This simply involves a list of messages\n",
    "# We want steps to return messages to append to the list\n",
    "# So we annotate the messages attribute with `add_messages` reducer\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "    is_last_step: IsLastStep\n",
    "\n",
    "    remaining_steps: RemainingSteps\n",
    "\n",
    "    structured_response: StructuredResponse\n",
    "\n",
    "\n",
    "StateSchema = TypeVar(\"StateSchema\", bound=AgentState)\n",
    "StateSchemaType = Type[StateSchema]\n",
    "\n",
    "PROMPT_RUNNABLE_NAME = \"Prompt\"\n",
    "\n",
    "MessagesModifier = Union[\n",
    "    SystemMessage,\n",
    "    str,\n",
    "    Callable[[Sequence[BaseMessage]], Sequence[BaseMessage]],\n",
    "    Runnable[Sequence[BaseMessage], Sequence[BaseMessage]],\n",
    "]\n",
    "\n",
    "Prompt = Union[\n",
    "    SystemMessage,\n",
    "    str,\n",
    "    Callable[[StateSchema], Sequence[BaseMessage]],\n",
    "    Runnable[StateSchema, Sequence[BaseMessage]],\n",
    "]\n",
    "\n",
    "\n",
    "def _get_prompt_runnable(prompt: Optional[Prompt]) -> Runnable:\n",
    "    prompt_runnable: Runnable\n",
    "    if prompt is None:\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            lambda state: state[\"messages\"], name=PROMPT_RUNNABLE_NAME\n",
    "        )\n",
    "    elif isinstance(prompt, str):\n",
    "        _system_message: BaseMessage = SystemMessage(content=prompt)\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            lambda state: [_system_message] + state[\"messages\"],\n",
    "            name=PROMPT_RUNNABLE_NAME,\n",
    "        )\n",
    "    elif isinstance(prompt, SystemMessage):\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            lambda state: [prompt] + state[\"messages\"],\n",
    "            name=PROMPT_RUNNABLE_NAME,\n",
    "        )\n",
    "    elif inspect.iscoroutinefunction(prompt):\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            None,\n",
    "            prompt,\n",
    "            name=PROMPT_RUNNABLE_NAME,\n",
    "        )\n",
    "    elif callable(prompt):\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            prompt,\n",
    "            name=PROMPT_RUNNABLE_NAME,\n",
    "        )\n",
    "    elif isinstance(prompt, Runnable):\n",
    "        prompt_runnable = prompt\n",
    "    else:\n",
    "        raise ValueError(f\"Got unexpected type for `prompt`: {type(prompt)}\")\n",
    "\n",
    "    return prompt_runnable\n",
    "\n",
    "\n",
    "def _convert_messages_modifier_to_prompt(\n",
    "    messages_modifier: MessagesModifier,\n",
    ") -> Prompt:\n",
    "    prompt: Prompt\n",
    "    if isinstance(messages_modifier, (str, SystemMessage)):\n",
    "        return messages_modifier\n",
    "    elif callable(messages_modifier):\n",
    "\n",
    "        def prompt(state: AgentState) -> Sequence[BaseMessage]:\n",
    "            return messages_modifier(state[\"messages\"])\n",
    "\n",
    "        return prompt\n",
    "    elif isinstance(messages_modifier, Runnable):\n",
    "        prompt = (lambda state: state[\"messages\"]) | messages_modifier\n",
    "        return prompt\n",
    "    raise ValueError(\n",
    "        f\"Got unexpected type for `messages_modifier`: {type(messages_modifier)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _convert_modifier_to_prompt(func: F) -> F:\n",
    "    \"\"\"Decorator that converts state_modifier/messages_modifier kwargs to prompt kwarg.\"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
    "        prompt = kwargs.get(\"prompt\")\n",
    "        state_modifier = kwargs.pop(\"state_modifier\", None)\n",
    "        messages_modifier = kwargs.pop(\"messages_modifier\", None)\n",
    "        if sum(p is not None for p in (prompt, state_modifier, messages_modifier)) > 1:\n",
    "            raise ValueError(\n",
    "                \"Expected only one of prompt, state_modifier, or messages_modifier, got multiple values\"\n",
    "            )\n",
    "\n",
    "        if state_modifier is not None:\n",
    "            prompt = state_modifier\n",
    "        elif messages_modifier is not None:\n",
    "            prompt = _convert_messages_modifier_to_prompt(messages_modifier)\n",
    "\n",
    "        kwargs[\"prompt\"] = prompt\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return cast(F, wrapper)\n",
    "\n",
    "\n",
    "def _should_bind_tools(model: LanguageModelLike, tools: Sequence[BaseTool]) -> bool:\n",
    "    if not isinstance(model, RunnableBinding):\n",
    "        return True\n",
    "\n",
    "    if \"tools\" not in model.kwargs:\n",
    "        return True\n",
    "\n",
    "    bound_tools = model.kwargs[\"tools\"]\n",
    "    if len(tools) != len(bound_tools):\n",
    "        raise ValueError(\n",
    "            \"Number of tools in the model.bind_tools() and tools passed to create_react_agent must match\"\n",
    "        )\n",
    "\n",
    "    tool_names = set(tool.name for tool in tools)\n",
    "    bound_tool_names = set()\n",
    "    for bound_tool in bound_tools:\n",
    "        # OpenAI-style tool\n",
    "        if bound_tool.get(\"type\") == \"function\":\n",
    "            bound_tool_name = bound_tool[\"function\"][\"name\"]\n",
    "        # Anthropic-style tool\n",
    "        elif bound_tool.get(\"name\"):\n",
    "            bound_tool_name = bound_tool[\"name\"]\n",
    "        else:\n",
    "            # unknown tool type so we'll ignore it\n",
    "            continue\n",
    "\n",
    "        bound_tool_names.add(bound_tool_name)\n",
    "\n",
    "    if missing_tools := tool_names - bound_tool_names:\n",
    "        raise ValueError(f\"Missing tools '{missing_tools}' in the model.bind_tools()\")\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_model(model: LanguageModelLike) -> BaseChatModel:\n",
    "    \"\"\"Get the underlying model from a RunnableBinding or return the model itself.\"\"\"\n",
    "    if isinstance(model, RunnableBinding):\n",
    "        model = model.bound\n",
    "\n",
    "    if not isinstance(model, BaseChatModel):\n",
    "        raise TypeError(\n",
    "            f\"Expected `model` to be a ChatModel or RunnableBinding (e.g. model.bind_tools(...)), got {type(model)}\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _validate_chat_history(\n",
    "    messages: Sequence[BaseMessage],\n",
    ") -> None:\n",
    "    \"\"\"Validate that all tool calls in AIMessages have a corresponding ToolMessage.\"\"\"\n",
    "    all_tool_calls = [\n",
    "        tool_call\n",
    "        for message in messages\n",
    "        if isinstance(message, AIMessage)\n",
    "        for tool_call in message.tool_calls\n",
    "    ]\n",
    "    tool_call_ids_with_results = {\n",
    "        message.tool_call_id for message in messages if isinstance(message, ToolMessage)\n",
    "    }\n",
    "    tool_calls_without_results = [\n",
    "        tool_call\n",
    "        for tool_call in all_tool_calls\n",
    "        if tool_call[\"id\"] not in tool_call_ids_with_results\n",
    "    ]\n",
    "    if not tool_calls_without_results:\n",
    "        return\n",
    "\n",
    "    error_message = create_error_message(\n",
    "        message=\"Found AIMessages with tool_calls that do not have a corresponding ToolMessage. \"\n",
    "        f\"Here are the first few of those tool calls: {tool_calls_without_results[:3]}.\\n\\n\"\n",
    "        \"Every tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage \"\n",
    "        \"(result of a tool invocation to return to the LLM) - this is required by most LLM providers.\",\n",
    "        error_code=ErrorCode.INVALID_CHAT_HISTORY,\n",
    "    )\n",
    "    raise ValueError(error_message)\n",
    "\n",
    "\n",
    "@_convert_modifier_to_prompt\n",
    "def create_react_agent(\n",
    "    model: Union[str, LanguageModelLike],\n",
    "    tools: Union[ToolExecutor, Sequence[BaseTool], ToolNode],\n",
    "    *,\n",
    "    state_schema: Optional[StateSchemaType] = None,\n",
    "    prompt: Optional[Prompt] = None,\n",
    "    response_format: Optional[\n",
    "        Union[StructuredResponseSchema, tuple[str, StructuredResponseSchema]]\n",
    "    ] = None,\n",
    "    checkpointer: Optional[Checkpointer] = None,\n",
    "    store: Optional[BaseStore] = None,\n",
    "    interrupt_before: Optional[list[str]] = None,\n",
    "    interrupt_after: Optional[list[str]] = None,\n",
    "    debug: bool = False,\n",
    "    version: Literal[\"v1\", \"v2\"] = \"v1\",\n",
    "    name: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    \"\"\"Creates a graph that works with a chat model that utilizes tool calling.\n",
    "\n",
    "    Args:\n",
    "        model: The `LangChain` chat model that supports tool calling.\n",
    "        tools: A list of tools, a ToolExecutor, or a ToolNode instance.\n",
    "            If an empty list is provided, the agent will consist of a single LLM node without tool calling.\n",
    "        state_schema: An optional state schema that defines graph state.\n",
    "            Must have `messages` and `is_last_step` keys.\n",
    "            Defaults to `AgentState` that defines those two keys.\n",
    "        prompt: An optional prompt for the LLM. Can take a few different forms:\n",
    "\n",
    "            - str: This is converted to a SystemMessage and added to the beginning of the list of messages in state[\"messages\"].\n",
    "            - SystemMessage: this is added to the beginning of the list of messages in state[\"messages\"].\n",
    "            - Callable: This function should take in full graph state and the output is then passed to the language model.\n",
    "            - Runnable: This runnable should take in full graph state and the output is then passed to the language model.\n",
    "\n",
    "            !!! Note\n",
    "                Prior to `v0.2.68`, the prompt was set using `state_modifier` / `messages_modifier` parameters.\n",
    "        response_format: An optional schema for the final agent output.\n",
    "\n",
    "            If provided, output will be formatted to match the given schema and returned in the 'structured_response' state key.\n",
    "            If not provided, `structured_response` will not be present in the output state.\n",
    "            Can be passed in as:\n",
    "\n",
    "                - an OpenAI function/tool schema,\n",
    "                - a JSON Schema,\n",
    "                - a TypedDict class,\n",
    "                - or a Pydantic class.\n",
    "                - a tuple (prompt, schema), where schema is one of the above.\n",
    "                    The prompt will be used together with the model that is being used to generate the structured response.\n",
    "\n",
    "            !!! Important\n",
    "                `response_format` requires the model to support `.with_structured_output`\n",
    "\n",
    "            !!! Note\n",
    "                The graph will make a separate call to the LLM to generate the structured response after the agent loop is finished.\n",
    "                This is not the only strategy to get structured responses, see more options in [this guide](https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output/).\n",
    "        checkpointer: An optional checkpoint saver object. This is used for persisting\n",
    "            the state of the graph (e.g., as chat memory) for a single thread (e.g., a single conversation).\n",
    "        store: An optional store object. This is used for persisting data\n",
    "            across multiple threads (e.g., multiple conversations / users).\n",
    "        interrupt_before: An optional list of node names to interrupt before.\n",
    "            Should be one of the following: \"agent\", \"tools\".\n",
    "            This is useful if you want to add a user confirmation or other interrupt before taking an action.\n",
    "        interrupt_after: An optional list of node names to interrupt after.\n",
    "            Should be one of the following: \"agent\", \"tools\".\n",
    "            This is useful if you want to return directly or run additional processing on an output.\n",
    "        debug: A flag indicating whether to enable debug mode.\n",
    "        version: Determines the version of the graph to create.\n",
    "            Can be one of:\n",
    "\n",
    "            - `\"v1\"`: The tool node processes a single message. All tool\n",
    "                calls in the message are executed in parallel within the tool node.\n",
    "            - `\"v2\"`: The tool node processes a tool call.\n",
    "                Tool calls are distributed across multiple instances of the tool\n",
    "                node using the [Send](https://langchain-ai.github.io/langgraph/concepts/low_level/#send)\n",
    "                API.\n",
    "        name: An optional name for the CompiledStateGraph.\n",
    "            This name will be automatically used when adding ReAct agent graph to another graph as a subgraph node -\n",
    "            particularly useful for building multi-agent systems.\n",
    "\n",
    "    Returns:\n",
    "        A compiled LangChain runnable that can be used for chat interactions.\n",
    "\n",
    "    The resulting graph looks like this:\n",
    "\n",
    "    ``` mermaid\n",
    "    stateDiagram-v2\n",
    "        [*] --> Start\n",
    "        Start --> Agent\n",
    "        Agent --> Tools : continue\n",
    "        Tools --> Agent\n",
    "        Agent --> End : end\n",
    "        End --> [*]\n",
    "\n",
    "        classDef startClass fill:#ffdfba;\n",
    "        classDef endClass fill:#baffc9;\n",
    "        classDef otherClass fill:#fad7de;\n",
    "\n",
    "        class Start startClass\n",
    "        class End endClass\n",
    "        class Agent,Tools otherClass\n",
    "    ```\n",
    "\n",
    "    The \"agent\" node calls the language model with the messages list (after applying the messages modifier).\n",
    "    If the resulting AIMessage contains `tool_calls`, the graph will then call the [\"tools\"][langgraph.prebuilt.tool_node.ToolNode].\n",
    "    The \"tools\" node executes the tools (1 tool per `tool_call`) and adds the responses to the messages list\n",
    "    as `ToolMessage` objects. The agent node then calls the language model again.\n",
    "    The process repeats until no more `tool_calls` are present in the response.\n",
    "    The agent then returns the full list of messages as a dictionary containing the key \"messages\".\n",
    "\n",
    "    ``` mermaid\n",
    "        sequenceDiagram\n",
    "            participant U as User\n",
    "            participant A as Agent (LLM)\n",
    "            participant T as Tools\n",
    "            U->>A: Initial input\n",
    "            Note over A: Messages modifier + LLM\n",
    "            loop while tool_calls present\n",
    "                A->>T: Execute tools\n",
    "                T-->>A: ToolMessage for each tool_calls\n",
    "            end\n",
    "            A->>U: Return final state\n",
    "    ```\n",
    "\n",
    "    Examples:\n",
    "        Use with a simple tool:\n",
    "\n",
    "        ```pycon\n",
    "        >>> from datetime import datetime\n",
    "        >>> from langchain_openai import ChatOpenAI\n",
    "        >>> from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "        ... def check_weather(location: str, at_time: datetime | None = None) -> str:\n",
    "        ...     '''Return the weather forecast for the specified location.'''\n",
    "        ...     return f\"It's always sunny in {location}\"\n",
    "        >>>\n",
    "        >>> tools = [check_weather]\n",
    "        >>> model = ChatOpenAI(model=\"gpt-4o\")\n",
    "        >>> graph = create_react_agent(model, tools=tools)\n",
    "        >>> inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n",
    "        >>> for s in graph.stream(inputs, stream_mode=\"values\"):\n",
    "        ...     message = s[\"messages\"][-1]\n",
    "        ...     if isinstance(message, tuple):\n",
    "        ...         print(message)\n",
    "        ...     else:\n",
    "        ...         message.pretty_print()\n",
    "        ('user', 'what is the weather in sf')\n",
    "        ================================== Ai Message ==================================\n",
    "        Tool Calls:\n",
    "        check_weather (call_LUzFvKJRuaWQPeXvBOzwhQOu)\n",
    "        Call ID: call_LUzFvKJRuaWQPeXvBOzwhQOu\n",
    "        Args:\n",
    "            location: San Francisco\n",
    "        ================================= Tool Message =================================\n",
    "        Name: check_weather\n",
    "        It's always sunny in San Francisco\n",
    "        ================================== Ai Message ==================================\n",
    "        The weather in San Francisco is sunny.\n",
    "        ```\n",
    "        Add a system prompt for the LLM:\n",
    "\n",
    "        ```pycon\n",
    "        >>> system_prompt = \"You are a helpful bot named Fred.\"\n",
    "        >>> graph = create_react_agent(model, tools, prompt=system_prompt)\n",
    "        >>> inputs = {\"messages\": [(\"user\", \"What's your name? And what's the weather in SF?\")]}\n",
    "        >>> for s in graph.stream(inputs, stream_mode=\"values\"):\n",
    "        ...     message = s[\"messages\"][-1]\n",
    "        ...     if isinstance(message, tuple):\n",
    "        ...         print(message)\n",
    "        ...     else:\n",
    "        ...         message.pretty_print()\n",
    "        ('user', \"What's your name? And what's the weather in SF?\")\n",
    "        ================================== Ai Message ==================================\n",
    "        Hi, my name is Fred. Let me check the weather in San Francisco for you.\n",
    "        Tool Calls:\n",
    "        check_weather (call_lqhj4O0hXYkW9eknB4S41EXk)\n",
    "        Call ID: call_lqhj4O0hXYkW9eknB4S41EXk\n",
    "        Args:\n",
    "            location: San Francisco\n",
    "        ================================= Tool Message =================================\n",
    "        Name: check_weather\n",
    "        It's always sunny in San Francisco\n",
    "        ================================== Ai Message ==================================\n",
    "        The weather in San Francisco is currently sunny. If you need any more details or have other questions, feel free to ask!\n",
    "        ```\n",
    "\n",
    "        Add a more complex prompt for the LLM:\n",
    "\n",
    "        ```pycon\n",
    "        >>> from langchain_core.prompts import ChatPromptTemplate\n",
    "        >>> prompt = ChatPromptTemplate.from_messages([\n",
    "        ...     (\"system\", \"You are a helpful bot named Fred.\"),\n",
    "        ...     (\"placeholder\", \"{messages}\"),\n",
    "        ...     (\"user\", \"Remember, always be polite!\"),\n",
    "        ... ])\n",
    "        >>>\n",
    "        >>> graph = create_react_agent(model, tools, prompt=prompt)\n",
    "        >>> inputs = {\"messages\": [(\"user\", \"What's your name? And what's the weather in SF?\")]}\n",
    "        >>> for s in graph.stream(inputs, stream_mode=\"values\"):\n",
    "        ...     message = s[\"messages\"][-1]\n",
    "        ...     if isinstance(message, tuple):\n",
    "        ...         print(message)\n",
    "        ...     else:\n",
    "        ...         message.pretty_print()\n",
    "        ```\n",
    "\n",
    "        Add complex prompt with custom graph state:\n",
    "\n",
    "        ```pycon\n",
    "        >>> from typing_extensions import TypedDict\n",
    "        >>>\n",
    "        >>> from langgraph.managed import IsLastStep\n",
    "        >>> prompt = ChatPromptTemplate.from_messages(\n",
    "        ...     [\n",
    "        ...         (\"system\", \"Today is {today}\"),\n",
    "        ...         (\"placeholder\", \"{messages}\"),\n",
    "        ...     ]\n",
    "        ... )\n",
    "        >>>\n",
    "        >>> class CustomState(TypedDict):\n",
    "        ...     today: str\n",
    "        ...     messages: Annotated[list[BaseMessage], add_messages]\n",
    "        ...     is_last_step: IsLastStep\n",
    "        >>>\n",
    "        >>> graph = create_react_agent(model, tools, state_schema=CustomState, prompt=prompt)\n",
    "        >>> inputs = {\"messages\": [(\"user\", \"What's today's date? And what's the weather in SF?\")], \"today\": \"July 16, 2004\"}\n",
    "        >>> for s in graph.stream(inputs, stream_mode=\"values\"):\n",
    "        ...     message = s[\"messages\"][-1]\n",
    "        ...     if isinstance(message, tuple):\n",
    "        ...         print(message)\n",
    "        ...     else:\n",
    "        ...         message.pretty_print()\n",
    "        ```\n",
    "\n",
    "        Add thread-level \"chat memory\" to the graph:\n",
    "\n",
    "        ```pycon\n",
    "        >>> from langgraph.checkpoint.memory import MemorySaver\n",
    "        >>> graph = create_react_agent(model, tools, checkpointer=MemorySaver())\n",
    "        >>> config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
    "        >>> def print_stream(graph, inputs, config):\n",
    "        ...     for s in graph.stream(inputs, config, stream_mode=\"values\"):\n",
    "        ...         message = s[\"messages\"][-1]\n",
    "        ...         if isinstance(message, tuple):\n",
    "        ...             print(message)\n",
    "        ...         else:\n",
    "        ...             message.pretty_print()\n",
    "        >>> inputs = {\"messages\": [(\"user\", \"What's the weather in SF?\")]}\n",
    "        >>> print_stream(graph, inputs, config)\n",
    "        >>> inputs2 = {\"messages\": [(\"user\", \"Cool, so then should i go biking today?\")]}\n",
    "        >>> print_stream(graph, inputs2, config)\n",
    "        ('user', \"What's the weather in SF?\")\n",
    "        ================================== Ai Message ==================================\n",
    "        Tool Calls:\n",
    "        check_weather (call_ChndaktJxpr6EMPEB5JfOFYc)\n",
    "        Call ID: call_ChndaktJxpr6EMPEB5JfOFYc\n",
    "        Args:\n",
    "            location: San Francisco\n",
    "        ================================= Tool Message =================================\n",
    "        Name: check_weather\n",
    "        It's always sunny in San Francisco\n",
    "        ================================== Ai Message ==================================\n",
    "        The weather in San Francisco is sunny. Enjoy your day!\n",
    "        ================================ Human Message =================================\n",
    "        Cool, so then should i go biking today?\n",
    "        ================================== Ai Message ==================================\n",
    "        Since the weather in San Francisco is sunny, it sounds like a great day for biking! Enjoy your ride!\n",
    "        ```\n",
    "\n",
    "        Add an interrupt to let the user confirm before taking an action:\n",
    "\n",
    "        ```pycon\n",
    "        >>> graph = create_react_agent(\n",
    "        ...     model, tools, interrupt_before=[\"tools\"], checkpointer=MemorySaver()\n",
    "        >>> )\n",
    "        >>> config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
    "\n",
    "        >>> inputs = {\"messages\": [(\"user\", \"What's the weather in SF?\")]}\n",
    "        >>> print_stream(graph, inputs, config)\n",
    "        >>> snapshot = graph.get_state(config)\n",
    "        >>> print(\"Next step: \", snapshot.next)\n",
    "        >>> print_stream(graph, None, config)\n",
    "        ```\n",
    "\n",
    "        Add cross-thread memory to the graph:\n",
    "\n",
    "        ```pycon\n",
    "        >>> from langgraph.prebuilt import InjectedStore\n",
    "        >>> from langgraph.store.base import BaseStore\n",
    "\n",
    "        >>> def save_memory(memory: str, *, config: RunnableConfig, store: Annotated[BaseStore, InjectedStore()]) -> str:\n",
    "        ...     '''Save the given memory for the current user.'''\n",
    "        ...     # This is a **tool** the model can use to save memories to storage\n",
    "        ...     user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
    "        ...     namespace = (\"memories\", user_id)\n",
    "        ...     store.put(namespace, f\"memory_{len(store.search(namespace))}\", {\"data\": memory})\n",
    "        ...     return f\"Saved memory: {memory}\"\n",
    "\n",
    "        >>> def prepare_model_inputs(state: AgentState, config: RunnableConfig, store: BaseStore):\n",
    "        ...     # Retrieve user memories and add them to the system message\n",
    "        ...     # This function is called **every time** the model is prompted. It converts the state to a prompt\n",
    "        ...     user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
    "        ...     namespace = (\"memories\", user_id)\n",
    "        ...     memories = [m.value[\"data\"] for m in store.search(namespace)]\n",
    "        ...     system_msg = f\"User memories: {', '.join(memories)}\"\n",
    "        ...     return [{\"role\": \"system\", \"content\": system_msg)] + state[\"messages\"]\n",
    "\n",
    "        >>> from langgraph.checkpoint.memory import MemorySaver\n",
    "        >>> from langgraph.store.memory import InMemoryStore\n",
    "        >>> store = InMemoryStore()\n",
    "        >>> graph = create_react_agent(model, [save_memory], prompt=prepare_model_inputs, store=store, checkpointer=MemorySaver())\n",
    "        >>> config = {\"configurable\": {\"thread_id\": \"thread-1\", \"user_id\": \"1\"}}\n",
    "\n",
    "        >>> inputs = {\"messages\": [(\"user\", \"Hey I'm Will, how's it going?\")]}\n",
    "        >>> print_stream(graph, inputs, config)\n",
    "        ('user', \"Hey I'm Will, how's it going?\")\n",
    "        ================================== Ai Message ==================================\n",
    "        Hello Will! It's nice to meet you. I'm doing well, thank you for asking. How are you doing today?\n",
    "\n",
    "        >>> inputs2 = {\"messages\": [(\"user\", \"I like to bike\")]}\n",
    "        >>> print_stream(graph, inputs2, config)\n",
    "        ================================ Human Message =================================\n",
    "        I like to bike\n",
    "        ================================== Ai Message ==================================\n",
    "        That's great to hear, Will! Biking is an excellent hobby and form of exercise. It's a fun way to stay active and explore your surroundings. Do you have any favorite biking routes or trails you enjoy? Or perhaps you're into a specific type of biking, like mountain biking or road cycling?\n",
    "\n",
    "        >>> config = {\"configurable\": {\"thread_id\": \"thread-2\", \"user_id\": \"1\"}}\n",
    "        >>> inputs3 = {\"messages\": [(\"user\", \"Hi there! Remember me?\")]}\n",
    "        >>> print_stream(graph, inputs3, config)\n",
    "        ================================ Human Message =================================\n",
    "        Hi there! Remember me?\n",
    "        ================================== Ai Message ==================================\n",
    "        User memories:\n",
    "        Hello! Of course, I remember you, Will! You mentioned earlier that you like to bike. It's great to hear from you again. How have you been? Have you been on any interesting bike rides lately?\n",
    "        ```\n",
    "\n",
    "        Add a timeout for a given step:\n",
    "\n",
    "        ```pycon\n",
    "        >>> import time\n",
    "        ... def check_weather(location: str, at_time: datetime | None = None) -> float:\n",
    "        ...     '''Return the weather forecast for the specified location.'''\n",
    "        ...     time.sleep(2)\n",
    "        ...     return f\"It's always sunny in {location}\"\n",
    "        >>>\n",
    "        >>> tools = [check_weather]\n",
    "        >>> graph = create_react_agent(model, tools)\n",
    "        >>> graph.step_timeout = 1 # Seconds\n",
    "        >>> for s in graph.stream({\"messages\": [(\"user\", \"what is the weather in sf\")]}):\n",
    "        ...     print(s)\n",
    "        TimeoutError: Timed out at step 2\n",
    "        ```\n",
    "    \"\"\"\n",
    "    if version not in (\"v1\", \"v2\"):\n",
    "        raise ValueError(\n",
    "            f\"Invalid version {version}. Supported versions are 'v1' and 'v2'.\"\n",
    "        )\n",
    "\n",
    "    if state_schema is not None:\n",
    "        required_keys = {\"messages\", \"remaining_steps\"}\n",
    "        if response_format is not None:\n",
    "            required_keys.add(\"structured_response\")\n",
    "\n",
    "        if missing_keys := required_keys - set(state_schema.__annotations__):\n",
    "            raise ValueError(f\"Missing required key(s) {missing_keys} in state_schema\")\n",
    "\n",
    "    if isinstance(tools, ToolExecutor):\n",
    "        tool_classes: Sequence[BaseTool] = tools.tools\n",
    "        tool_node = ToolNode(tool_classes)\n",
    "    elif isinstance(tools, ToolNode):\n",
    "        tool_classes = list(tools.tools_by_name.values())\n",
    "        tool_node = tools\n",
    "    else:\n",
    "        tool_node = ToolNode(tools)\n",
    "        # get the tool functions wrapped in a tool class from the ToolNode\n",
    "        tool_classes = list(tool_node.tools_by_name.values())\n",
    "\n",
    "    if isinstance(model, str):\n",
    "        try:\n",
    "            from langchain.chat_models import (  # type: ignore[import-not-found]\n",
    "                init_chat_model,\n",
    "            )\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install langchain (`pip install langchain`) to use '<provider>:<model>' string syntax for `model` parameter.\"\n",
    "            )\n",
    "\n",
    "        model = cast(BaseChatModel, init_chat_model(model))\n",
    "\n",
    "    tool_calling_enabled = len(tool_classes) > 0\n",
    "\n",
    "    if _should_bind_tools(model, tool_classes) and tool_calling_enabled:\n",
    "        model = cast(BaseChatModel, model).bind_tools(tool_classes)\n",
    "\n",
    "    model_runnable = _get_prompt_runnable(prompt) | model\n",
    "\n",
    "    # If any of the tools are configured to return_directly after running,\n",
    "    # our graph needs to check if these were called\n",
    "    should_return_direct = {t.name for t in tool_classes if t.return_direct}\n",
    "\n",
    "    # Define the function that calls the model\n",
    "    def call_model(state: AgentState, config: RunnableConfig) -> AgentState:\n",
    "        _validate_chat_history(state[\"messages\"])\n",
    "        response = cast(AIMessage, model_runnable.invoke(state, config))\n",
    "        # add agent name to the AIMessage\n",
    "        response.name = name\n",
    "        has_tool_calls = isinstance(response, AIMessage) and response.tool_calls\n",
    "        all_tools_return_direct = (\n",
    "            all(call[\"name\"] in should_return_direct for call in response.tool_calls)\n",
    "            if isinstance(response, AIMessage)\n",
    "            else False\n",
    "        )\n",
    "        if (\n",
    "            (\n",
    "                \"remaining_steps\" not in state\n",
    "                and state.get(\"is_last_step\", False)\n",
    "                and has_tool_calls\n",
    "            )\n",
    "            or (\n",
    "                \"remaining_steps\" in state\n",
    "                and state[\"remaining_steps\"] < 1\n",
    "                and all_tools_return_direct\n",
    "            )\n",
    "            or (\n",
    "                \"remaining_steps\" in state\n",
    "                and state[\"remaining_steps\"] < 2\n",
    "                and has_tool_calls\n",
    "            )\n",
    "        ):\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        id=response.id,\n",
    "                        content=\"Sorry, need more steps to process this request.\",\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        # We return a list, because this will get added to the existing list\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    async def acall_model(state: AgentState, config: RunnableConfig) -> AgentState:\n",
    "        _validate_chat_history(state[\"messages\"])\n",
    "        response = cast(AIMessage, await model_runnable.ainvoke(state, config))\n",
    "        # add agent name to the AIMessage\n",
    "        response.name = name\n",
    "        has_tool_calls = isinstance(response, AIMessage) and response.tool_calls\n",
    "        all_tools_return_direct = (\n",
    "            all(call[\"name\"] in should_return_direct for call in response.tool_calls)\n",
    "            if isinstance(response, AIMessage)\n",
    "            else False\n",
    "        )\n",
    "        if (\n",
    "            (\n",
    "                \"remaining_steps\" not in state\n",
    "                and state.get(\"is_last_step\", False)\n",
    "                and has_tool_calls\n",
    "            )\n",
    "            or (\n",
    "                \"remaining_steps\" in state\n",
    "                and state[\"remaining_steps\"] < 1\n",
    "                and all_tools_return_direct\n",
    "            )\n",
    "            or (\n",
    "                \"remaining_steps\" in state\n",
    "                and state[\"remaining_steps\"] < 2\n",
    "                and has_tool_calls\n",
    "            )\n",
    "        ):\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        id=response.id,\n",
    "                        content=\"Sorry, need more steps to process this request.\",\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        # We return a list, because this will get added to the existing list\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    def generate_structured_response(\n",
    "        state: AgentState, config: RunnableConfig\n",
    "    ) -> AgentState:\n",
    "        # NOTE: we exclude the last message because there is enough information\n",
    "        # for the LLM to generate the structured response\n",
    "        messages = state[\"messages\"][:-1]\n",
    "        structured_response_schema = response_format\n",
    "        if isinstance(response_format, tuple):\n",
    "            system_prompt, structured_response_schema = response_format\n",
    "            messages = [SystemMessage(content=system_prompt)] + list(messages)\n",
    "\n",
    "        model_with_structured_output = _get_model(model).with_structured_output(\n",
    "            cast(StructuredResponseSchema, structured_response_schema)\n",
    "        )\n",
    "        response = model_with_structured_output.invoke(messages, config)\n",
    "        return {\"structured_response\": response}\n",
    "\n",
    "    async def agenerate_structured_response(\n",
    "        state: AgentState, config: RunnableConfig\n",
    "    ) -> AgentState:\n",
    "        # NOTE: we exclude the last message because there is enough information\n",
    "        # for the LLM to generate the structured response\n",
    "        messages = state[\"messages\"][:-1]\n",
    "        structured_response_schema = response_format\n",
    "        if isinstance(response_format, tuple):\n",
    "            system_prompt, structured_response_schema = response_format\n",
    "            messages = [SystemMessage(content=system_prompt)] + list(messages)\n",
    "\n",
    "        model_with_structured_output = _get_model(model).with_structured_output(\n",
    "            cast(StructuredResponseSchema, structured_response_schema)\n",
    "        )\n",
    "        response = await model_with_structured_output.ainvoke(messages, config)\n",
    "        return {\"structured_response\": response}\n",
    "\n",
    "    if not tool_calling_enabled:\n",
    "        # Define a new graph\n",
    "        workflow = StateGraph(state_schema or AgentState)\n",
    "        workflow.add_node(\"agent\", RunnableCallable(call_model, acall_model))\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "        if response_format is not None:\n",
    "            workflow.add_node(\n",
    "                \"generate_structured_response\",\n",
    "                RunnableCallable(\n",
    "                    generate_structured_response, agenerate_structured_response\n",
    "                ),\n",
    "            )\n",
    "            workflow.add_edge(\"agent\", \"generate_structured_response\")\n",
    "        graph = workflow.compile(\n",
    "            checkpointer=checkpointer,\n",
    "            store=store,\n",
    "            interrupt_before=interrupt_before,\n",
    "            interrupt_after=interrupt_after,\n",
    "            debug=debug,\n",
    "            name=name,\n",
    "        )\n",
    "        return graph\n",
    "\n",
    "    # Define the function that determines whether to continue or not\n",
    "    def should_continue(state: AgentState) -> Union[str, list]:\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there is no function call, then we finish\n",
    "        if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
    "            return END if response_format is None else \"generate_structured_response\"\n",
    "        # Otherwise if there is, we continue\n",
    "        else:\n",
    "            if version == \"v1\":\n",
    "                return \"tools\"\n",
    "            elif version == \"v2\":\n",
    "                tool_calls = [\n",
    "                    tool_node.inject_tool_args(call, state, store)  # type: ignore[arg-type]\n",
    "                    for call in last_message.tool_calls\n",
    "                ]\n",
    "                return [Send(\"tools\", [tool_call]) for tool_call in tool_calls]\n",
    "\n",
    "    # Define a new graph\n",
    "    workflow = StateGraph(state_schema or AgentState)\n",
    "\n",
    "    # Define the two nodes we will cycle between\n",
    "    workflow.add_node(\"agent\", RunnableCallable(call_model, acall_model))\n",
    "    workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "    # Set the entrypoint as `agent`\n",
    "    # This means that this node is the first one called\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "\n",
    "    # Add a structured output node if response_format is provided\n",
    "    if response_format is not None:\n",
    "        workflow.add_node(\n",
    "            \"generate_structured_response\",\n",
    "            RunnableCallable(\n",
    "                generate_structured_response, agenerate_structured_response\n",
    "            ),\n",
    "        )\n",
    "        workflow.add_edge(\"generate_structured_response\", END)\n",
    "        should_continue_destinations = [\"tools\", \"generate_structured_response\"]\n",
    "    else:\n",
    "        should_continue_destinations = [\"tools\", END]\n",
    "\n",
    "    # We now add a conditional edge\n",
    "    workflow.add_conditional_edges(\n",
    "        # First, we define the start node. We use `agent`.\n",
    "        # This means these are the edges taken after the `agent` node is called.\n",
    "        \"agent\",\n",
    "        # Next, we pass in the function that will determine which node is called next.\n",
    "        should_continue,\n",
    "        path_map=should_continue_destinations,\n",
    "    )\n",
    "\n",
    "    def route_tool_responses(state: AgentState) -> Literal[\"agent\", \"__end__\"]:\n",
    "        for m in reversed(state[\"messages\"]):\n",
    "            if not isinstance(m, ToolMessage):\n",
    "                break\n",
    "            if m.name in should_return_direct:\n",
    "                return END\n",
    "        return \"agent\"\n",
    "\n",
    "    if should_return_direct:\n",
    "        workflow.add_conditional_edges(\"tools\", route_tool_responses)\n",
    "    else:\n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    # Finally, we compile it!\n",
    "    # This compiles it into a LangChain Runnable,\n",
    "    # meaning you can use it as you would any other runnable\n",
    "    return workflow.compile(\n",
    "        checkpointer=checkpointer,\n",
    "        store=store,\n",
    "        interrupt_before=interrupt_before,\n",
    "        interrupt_after=interrupt_after,\n",
    "        debug=debug,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n",
    "# Keep for backwards compatibility\n",
    "create_tool_calling_executor = create_react_agent\n",
    "\n",
    "__all__ = [\n",
    "    \"create_react_agent\",\n",
    "    \"create_tool_calling_executor\",\n",
    "    \"AgentState\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshould_continue\u001b[39m(state: AgentState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mUnion\u001b[49m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m]:\n\u001b[0;32m      4\u001b[0m     messages \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m     last_message \u001b[38;5;241m=\u001b[39m messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Union' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def should_continue(state: AgentState) -> Union[str, list]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
    "        return END if response_format is None else \"generate_structured_response\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        if version == \"v1\":\n",
    "            return \"tools\"\n",
    "        elif version == \"v2\":\n",
    "            tool_calls = [\n",
    "                tool_node.inject_tool_args(call, state, store)  # type: ignore[arg-type]\n",
    "                for call in last_message.tool_calls\n",
    "            ]\n",
    "            return [Send(\"tools\", [tool_call]) for tool_call in tool_calls]\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema or AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", RunnableCallable(call_model, acall_model))\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add a structured output node if response_format is provided\n",
    "if response_format is not None:\n",
    "    workflow.add_node(\n",
    "        \"generate_structured_response\",\n",
    "        RunnableCallable(\n",
    "            generate_structured_response, agenerate_structured_response\n",
    "        ),\n",
    "    )\n",
    "    workflow.add_edge(\"generate_structured_response\", END)\n",
    "    should_continue_destinations = [\"tools\", \"generate_structured_response\"]\n",
    "else:\n",
    "    should_continue_destinations = [\"tools\", END]\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    path_map=should_continue_destinations,\n",
    ")\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(workflow.get_graph(xray=2).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StateGraph' object has no attribute 'get_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m display(Image(\u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m(xray\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mdraw_mermaid_png()))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StateGraph' object has no attribute 'get_graph'"
     ]
    }
   ],
   "source": [
    "display(Image(workflow.get_graph(xray=2).draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
